> # Effective Approaches to Attention-based Neural Machine Translation
> 

* <font size=5>
  Global attention : same as original attention
* Local attention : small window of context
</font>
<font size=3>
    + generate aligned position $p_t$ 
    + $c_t$ weighted average over the set of source hidden states within the window $[p_t-D, p_t+D]$; $D$ hyper-parameter
    + $a_t \in \R^{2D+1}$
    + two variants of model to determine $p_t$
      + _Monotonic alignment(**local-m**)_
        > $p_t = t$

        > $\pmb{a}_t(s) = align(\pmb{h}_t, \overline{\pmb{h}}_s) = \frac{exp(score(h_t, \overline{h}_s))}{\sum_{s' \in [p_t-D, p_t+D]}{exp(score(h_t, \overline{h}_{s'}))}}$
      + _Predictive alignment(**local-p**)_
        > $p_t = S \cdot sigmoid(\pmb{v}_p^T tanh(\pmb{W}_p \pmb{h}_t))$ where $\pmb{W}_p$ and $\pmb{v}_p$ are the model parameters to be learned during training

        >$p_t \in [0, S]$

        > To favor alignment points near $p_t$, place a Gaussian distribution near $p_t$

        > $\pmb{a}_t(s) = align(\pmb{h}_t, \overline{\pmb{h}}_s)\cdot exp(\frac{-(s-p_t)^2}{2\sigma^2})$ where $\sigma = \frac{D}{2}$
</font>
<font size=5>
* Input-feeding approach : attentional vectors $\widetilde{h}_t$ are fed as inputs to the next time steps to inform the model about past alignment decisions.
</font>
